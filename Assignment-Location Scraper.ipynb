{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99eaec0",
   "metadata": {},
   "source": [
    "# Assignment Prompt - Retail Store Location Scraper\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc1ca53e",
   "metadata": {},
   "source": [
    "#This code might take 3-5 minutes to get executed because I have used an API to get the latitude and longitude of each store\n",
    "# So for each store a seprate request is generated to get the latitude and longitude, so it takes a bit more time.\n",
    "# I have used spencers store list webpage. There are 21 stores information on 1 page, there are multiple pages but I have scrapped the data from one page olny. Similarly we can do for the rest of the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabfe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Here I have choosen the spencers as my favourite retail brand in India to extract the data \n",
    "url = 'http://www.spencersretail.com/store-list?page=2'\n",
    "\n",
    "# Send a GET request to the specified URL and store the response in a variable called 'response'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup and store the resulting object in a variable called 'soup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the store names, addresses, postal codes, localities, regions, contact numbers, and opening/closing times using their respective HTML tags and attributes\n",
    "store_name = soup.find_all(\"span\", class_=\"fn\")\n",
    "store_address = soup.find_all(\"span\", itemprop=\"streetAddress\")\n",
    "postal_code = soup.find_all(\"span\", class_=\"postal-code\")\n",
    "locality = soup.find_all(\"span\", class_=\"locality\")\n",
    "region = soup.find_all(\"span\", class_=\"region\")\n",
    "Contact = soup.find_all(\"span\", itemprop=\"telephone\")\n",
    "Time = soup.find_all(\"div\", class_=\"field-content\")\n",
    "\n",
    "# Extract the opening times and closing times from the 'Time' variable and store them in separate lists called 'Opens_at' and 'Closes_at'\n",
    "Opens_at=[]\n",
    "for i in range(1,len(Time),3):\n",
    "    Opens_at.append(Time[i])\n",
    "    \n",
    "Closes_at = []\n",
    "for i in range(2,len(Time),3):\n",
    "    Closes_at.append(Time[i])\n",
    "\n",
    "# Create a list called 'full_address' containing dictionaries with the full address for each store, I will use this address list further to get the latitute and longitude\n",
    "full_address=[]\n",
    "for i in range(len(store_name)):\n",
    "    full_address.append({\n",
    "        store_address[i].text.strip() + postal_code[i].text.strip() + locality[i].text.strip() + region[i].text.strip()       \n",
    "    })\n",
    "\n",
    "\n",
    "# Now I will use an API from the wesite \"Rapid API\", which will give us the latitude and longitude of the addresses of the stores\n",
    "\n",
    "# Set up the API endpoint URL and headers to access the TrueWay Geocoding API\n",
    "url = \"https://trueway-geocoding.p.rapidapi.com/Geocode\"\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": \"b2dcd3ea06msh60d0300569d7b6dp1f52a6jsn31f7da53e46b\",\n",
    "\t\"X-RapidAPI-Host\": \"trueway-geocoding.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "\n",
    "# Query the TrueWay Geocoding API for the latitude and longitude of each store using their full address and store the results in a list called 'lat_long'\n",
    "lat_long = []\n",
    "for i in range(len(full_address)):\n",
    "    querystring = {\"address\":full_address[i],\"language\":\"en\"}\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    lat_long.append((response.json()['results'][0][\"location\"]))\n",
    "                  \n",
    "\n",
    "# Create a list of dictionaries to store the extracted data for each store\n",
    "data = []\n",
    "for i in range(len(store_name)):\n",
    "    data.append({\n",
    "        \"Store Name\": store_name[i].text.strip(),\n",
    "        \"Store Address\": store_address[i].text.strip(),\n",
    "        \"Postal Code\": postal_code[i].text.strip(),\n",
    "        \"Locality\": locality[i].text.strip(),\n",
    "        \"Region\": region[i].text.strip(),\n",
    "        \"Contact\": Contact[i].text.strip(),\n",
    "        \"Opens At\": Opens_at[i].text.strip(),\n",
    "        \"Closes At\": Closes_at[i].text.strip(),\n",
    "        \"Lat-Long\": lat_long[i]\n",
    "    })\n",
    "\n",
    "# Write the data into a CSV file called 'spencers.csv'\n",
    "with open('spencers.csv', mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
